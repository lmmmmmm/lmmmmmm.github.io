(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{197:function(a,s,t){"use strict";t.r(s);var e=t(0),r=Object(e.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var a=this,s=a.$createElement,t=a._self._c||s;return t("div",{staticClass:"content"},[t("h1",{attrs:{id:"scrapy常用命令"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapy常用命令","aria-hidden":"true"}},[a._v("#")]),a._v(" Scrapy常用命令")]),a._v(" "),t("h2",{attrs:{id:"_1-创建项目"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-创建项目","aria-hidden":"true"}},[a._v("#")]),a._v(" 1 创建项目")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy startproject projectName\n")])])]),t("p",[a._v("创建成功后进入项目,项目结构如下所示:")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("├── scrapy.cfg\n└── testproject\n    ├── __init__.py\n    ├── items.py\n    ├── middlewares.py\n    ├── pipelines.py\n    ├── __pycache__\n    ├── settings.py\n    └── spiders\n        ├── __init__.py\n        └── __pycache__\n")])])]),t("h2",{attrs:{id:"_2-生成spider"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-生成spider","aria-hidden":"true"}},[a._v("#")]),a._v(" 2 生成Spider")]),a._v(" "),t("h3",{attrs:{id:"_2-1-基本生成方式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-基本生成方式","aria-hidden":"true"}},[a._v("#")]),a._v(" 2.1 基本生成方式")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy genspider baidu www.baidu.com\n")])])]),t("p",[a._v("该命令一共两个参数 "),t("strong",[a._v("scrapy genspider [args1] [agrs2]")]),a._v(":第一个参数将生成爬虫的名称,第二个参数表示要爬取的网站.执行完成后,将在spiders文件夹下生成一个python文件,名称则为刚生成spider的第一个参数.该文件如下所示:")]),a._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# -*- coding: utf-8 -*-")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" scrapy\n\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("class")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("BaiduSpider")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("scrapy"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("Spider"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    name "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'baidu'")]),a._v("\n    allowed_domains "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'www.baidu.com'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n    start_urls "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'http://www.baidu.com/'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("def")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("parse")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("self"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" response"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("pass")]),a._v("\n")])])]),t("h3",{attrs:{id:"_2-2-带模板的生成方式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-带模板的生成方式","aria-hidden":"true"}},[a._v("#")]),a._v(" 2.2 带模板的生成方式")]),a._v(" "),t("p",[a._v("在生成spider时scrapy为我们提供了模板:我们可以使用以下命令来查看:")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy genspider -l\n")])])]),t("p",[a._v("我们可以看到它提供了以下模板:")]),a._v(" "),t("ul",[t("li",[a._v("basic")]),a._v(" "),t("li",[a._v("crawl")]),a._v(" "),t("li",[a._v("csvfeed")]),a._v(" "),t("li",[a._v("xmlfeed"),t("br"),a._v("\n这样我们在生成spider时可以加上这些模板,如:")])]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy genspider -t crawl zhihu www.zhihu.com\n")])])]),t("p",[a._v("然后我们就可以在spider文件夹下看到zhihu.py文件")]),a._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# -*- coding: utf-8 -*-")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" scrapy\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" scrapy"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("linkextractors "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" LinkExtractor\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" scrapy"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spiders "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" CrawlSpider"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" Rule\n\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("class")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ZhihuSpider")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("CrawlSpider"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    name "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'zhihu'")]),a._v("\n    allowed_domains "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'www.zhihu.com'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n    start_urls "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'http://www.zhihu.com/'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("\n\n    rules "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("\n        Rule"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("LinkExtractor"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("allow"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("r"),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'Items/'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" callback"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'parse_item'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" follow"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("True")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("def")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("parse_item")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("self"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" response"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n        i "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#i['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').extract()")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#i['name'] = response.xpath('//div[@id=\"name\"]').extract()")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#i['description'] = response.xpath('//div[@id=\"description\"]').extract()")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("return")]),a._v(" i\n")])])]),t("h1",{attrs:{id:"_3-运行sprider"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-运行sprider","aria-hidden":"true"}},[a._v("#")]),a._v(" 3 运行sprider")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy crawl spidername\n")])])]),t("p",[a._v("spidername 为spider的名称,如刚生成的baidu,如果会打印出一些请求百度的日志,则说明Spider运行成功.")]),a._v(" "),t("h1",{attrs:{id:"_4-检查代码是否有误"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-检查代码是否有误","aria-hidden":"true"}},[a._v("#")]),a._v(" 4 检查代码是否有误")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy check\n")])])]),t("h1",{attrs:{id:"_5-查看项目中所有spider"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-查看项目中所有spider","aria-hidden":"true"}},[a._v("#")]),a._v(" 5 查看项目中所有spider")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("$ scrapy list\nbaidu\nzhihu\n")])])]),t("h1",{attrs:{id:"_6-fetch"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-fetch","aria-hidden":"true"}},[a._v("#")]),a._v(" 6 fetch")]),a._v(" "),t("p",[a._v("帮助我们访问一个网页,将源代码返还.如:")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy fetch http://www.baidu.com\n")])])]),t("h2",{attrs:{id:"_6-1支持参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-1支持参数","aria-hidden":"true"}},[a._v("#")]),a._v(" 6.1支持参数")]),a._v(" "),t("p",[a._v("加上nolog参数将会直接返还源代码,不返还一些请求信息")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy fetch --nolog http://www.baidu.com\n")])])]),t("p",[a._v("加上headers将会返还response Header")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v(" fetch  --headers  http://www.baidu.com\n")])])]),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n> User-Agent: Scrapy/1.5.2 (+https://scrapy.org)\n> Accept-Language: en\n> Accept-Encoding: gzip,deflate\n>\n< Last-Modified: Mon, 23 Jan 2017 13:27:36 GMT\n< Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform\n< Date: Tue, 29 Jan 2019 10:45:49 GMT\n< Content-Type: text/html\n< Server: bfe/1.0.8.18\n< Pragma: no-cache\n< Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/\n")])])]),t("h1",{attrs:{id:"_7-shell"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-shell","aria-hidden":"true"}},[a._v("#")]),a._v(" 7 shell")]),a._v(" "),t("p",[a._v("可以使我们以交互模式访问一个网站,如:")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("scrapy shell https://www.baidu.com\n")])])]),t("h1",{attrs:{id:"_8-获取当前配置信息"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_8-获取当前配置信息","aria-hidden":"true"}},[a._v("#")]),a._v(" 8 获取当前配置信息")]),a._v(" "),t("p",[a._v("获取settings.py中的配置信息:如")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v(" scrapy settings --get MONGO_DB\n")])])])])}],!1,null,null,null);r.options.__file="scrapy命令行.md";s.default=r.exports}}]);